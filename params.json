{
  "name": "Prediction of Student Alcohol Consumption Level Using Various Machine Learning Techniques",
  "tagline": "",
  "body": "## ABSTRACT\r\nThis project compares the prediction accuracies of different machine learning algorithms, for alcohol consumption level among school students. In this pursuit, three machine learning models, such as _Decision Tree, Support Vector Machine and Naïve-Bayes classifier_ are used on two real life data sets. Additionally, the importance of various features are deduced, that highly impact the prediction accuracy of an algorithm.\r\n\r\n## PROBLEM DESCRIPTION\r\n![flow_diagram](https://cloud.githubusercontent.com/assets/20998518/21067018/2184f420-be26-11e6-9546-a97d23f30e8a.png)\r\n\r\nRegular consumption of alcohol has adverse impacts on our physical and mental health. There are many factors that influence the drinking pattern of a person, for example, gender, upbringing, socio-economic state of the country etc. Hence, it is worthwhile to study such influencing factors, with a view to limiting the alcohol consumption of a society, at large. The challenges of this study are two-fold: (a) real life data-set containing several features that gives rise to a specific alcohol consumption level and (b) accurate mining of such data-set so as to correctly isolate the important factors that maximize the consumption. Few researchers in the past did surveys to collect the data sets, concerning the drinking pattern among teenagers. Hence, this project addresses the second challenge. This project considers two real life data sets, from the UCI Machine Learning Repository, and applies three machine learning algorithms (e.g., _Decision Tree, Support Vector Machine and Naïve-Bayes Classifier_) to predict the alcohol consumption, subjected to various factors. The prediction accuracies are computed in terms of the average accuracy from 10-fold cross validation. Further, the features that strongly influence the alcohol consumption are extracted using _Information Gain_ analysis from a Decision Tree model.\r\n\r\n## DATASET\r\n![dataset](https://cloud.githubusercontent.com/assets/20998518/21066843/073836fa-be25-11e6-8773-b5474ab52a9e.png)\r\n\r\nThe original data was integrated into two data-sets related to Mathematics (with 395 examples) and the Portuguese language (649 records) classes. Contributors: Paulo Cortez and Alice Silva.\r\n\r\n## METHODOLOGY\r\n### Data Pre-processing\r\n* **Finding a Binary Output**: The dataset has two features, viz., _Dalc_ (workday alcohol consumption) and _Walc_ (weekend alcohol consumption), both in the range of 1 (very low) to 5 (very high). Derived output: Alc = (Walc X 2 + Dalc X 5) / 7, again, in the range of 1 – 5. Derived binary output: _**Alc_bin = Alc < Th ? 0 : 1**_ (where 0 and 1 denote non-alcoholic and alcoholic, respectively). _**Different values of Th are explored (e.g., 2, 2.5 and 3).**_\r\n\r\n* **Filtering poorly correlated features**: Using correlation co-efficient, 4 features were discarded for which the correlation values are less than a given threshold.\r\n\r\n### Machine Learning Models\r\n* **Decision Tree (DT)**: ): _Information Gain_ based analysis is used for prediction and feature extraction. Random validation (RV) and 10 –fold Cross Validation (XV) techniques are used for determining prediction accuracy.\r\n\r\n* **Support Vector Machine (SVM)**: A multi-class SVM with C-support vector classification is considered. The default parameters for the libSVM tool are used. Two kernel types (e.g., Radial Basis Function (RBF) and Sigmoid) are explored. Original categorical features are converted into discrete features in a sparse format.\u000B\r\n\r\n* **Naïve-Bayes Classifier (NBC)**: Gaussian and Multinomial decision rules are explored. The results from Bernoulli decision rule are discarded due to extremely poor accuracy. Laplace smoothing parameter is used for the multinomial decision rule. Original categorical features are converted into discrete features in a non-sparse format.\r\n\r\n### Comparative Schemes\r\n![comp_scheme](https://cloud.githubusercontent.com/assets/20998518/21067986/1ff160de-be2c-11e6-9bb4-dd64be87ec8c.png)\r\n\r\n## EXPERIMENTAL RESULTS\r\n\r\n### Accuracy for the student of Mathematics class\r\n![pred_dt_mat](https://cloud.githubusercontent.com/assets/20998518/21065745/2a5003d0-be1f-11e6-8ed2-c283b78c3e06.png)\r\n\r\n### Accuracy for the student of Portuguese class\r\n![pred_dt_por](https://cloud.githubusercontent.com/assets/20998518/21065744/2a4c1ba8-be1f-11e6-8e6a-e69fa76e7f4d.png)\r\n\r\n### Accuracy Analysis\r\nConsidering a higher threshold (Th) for alcohol consumption to determine an alcoholic, increases the prediction accuracy for all the classifiers. For NBC, multinomial decision rule gives better accuracies than Gaussian. For SVM, RBF kernel achieves a better accuracy compared to Sigmoid kernel. The SVM, in general, offers the best accuracies amongst all the classifiers.\r\n\r\n### Important Feature Extraction\r\n![rank](https://cloud.githubusercontent.com/assets/20998518/21067142/a1435f3a-be26-11e6-8f46-1869795898c7.png)\r\n\r\nThe pi-chart above is created based on _Information Gain_ analysis on a Decision Tree model. A higher percentage indicates a greater importance. As each split in a decision tree is based on a maximum information gain decision, a split on a feature closer to the root of the tree must mean that the feature had more information gain than a split with a different feature lower in the tree. So, for a given feature F, that occurs in several splits within the tree, F's average distance away from the root has been calculated. All the features are then ranked by average distance, with the lowest average being the most valuable feature. The pi-chart indicates that sex, health and age are three most important features.\r\n\r\n## SUMMARY AND CONCLUSION\r\n* SVM classifier gives the best result in terms of average accuracy.\r\n* RBF kernel gives better accuracy with respect to Sigmoid kernel for SVM.\r\n* Multinomial rule gives better accuracy with respect to Gaussian for NBC.\r\n* In general, a higher accuracy is obtained for a larger value of _Th_.\r\n* Sex, health and age are the top three important features.\r\n\r\n## ACKNOWLEDGEMENT\r\nI thank Dr. Nicholas Flann for his valuable insight to this project.\r\n\r\n## REFERENCES\r\n* http://archive.ics.uci.edu/ml/datasets/STUDENT+ALCOHOL+CONSUMPTION#\r\n* libSVM, https://www.csie.ntu.edu.tw/~cjlin/libsvm/\r\n* Naïve-Bayes classifier, https://github.com/timnugent/naive-bayes\r\n* http://arstechnica.com/science/2016/03/drunk-tweeting-computer-algorithm/",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}